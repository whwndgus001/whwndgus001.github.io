## Integer Encoding
문장을 구성하는 단어들에 대해 숫자 부여하기 보통 ABC순 또는 빈도가 높은순으로 구성합니다.


```python
import nltk
nltk.download('punkt')
```

    [nltk_data] Downloading package punkt to /root/nltk_data...
    [nltk_data]   Unzipping tokenizers/punkt.zip.
    




    True




```python
from nltk.tokenize import sent_tokenize

text = """Isn't she lovely.
Isn't she wonderful.
Isn't she precious.
Less than one minute old.
I never thought through love we'd be.
Making one as lovely as she.
But isn't she lovely made from love.
Isn't she pretty.
Truly the angel's best.
Boy, I'm so happy.
We have been heaven blessed.
I can't believe what God has done.
Through us he's given life to one.
But isn't she lovely made from love.
Isn't she lovely.
Life and love are the same.
Life is Aisha.
The meaning of her name.
Londie, it could have not been done.
Without you who conceived the one.
That's so very lovely made from love."""

text = sent_tokenize(text)
print(text)
```

    ["Isn't she lovely.", "Isn't she wonderful.", "Isn't she precious.", 'Less than one minute old.', "I never thought through love we'd be.", 'Making one as lovely as she.', "But isn't she lovely made from love.", "Isn't she pretty.", "Truly the angel's best.", "Boy, I'm so happy.", 'We have been heaven blessed.', "I can't believe what God has done.", "Through us he's given life to one.", "But isn't she lovely made from love.", "Isn't she lovely.", 'Life and love are the same.', 'Life is Aisha.', 'The meaning of her name.', 'Londie, it could have not been done.', 'Without you who conceived the one.', "That's so very lovely made from love."]
    

## Word Tokenization


```python
nltk.download('stopwords')
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.
    




    True




```python
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

sentences = []
stop_words = set(stopwords.words('english')) # NLTK 불용어

# 문장별 단어 토큰화
for i in text:
  # print(i)
  sentence = word_tokenize(i) # 단어 토큰화
  result = []

  # 정제 작업 수행
  for word in sentence:
    word = word.lower() # 모든 단어의 알파벳을 소문자로 바꿔줍니다.

    # 불용어 제거하기
    if word not in stop_words:
      if len(word) > 2: # 단어의 길이가 2이하인 경우에도 단어를 제거
        result.append(word)

  sentences.append(result)
  
print(sentences)

```

    [["n't", 'lovely'], ["n't", 'wonderful'], ["n't", 'precious'], ['less', 'one', 'minute', 'old'], ['never', 'thought', 'love'], ['making', 'one', 'lovely'], ["n't", 'lovely', 'made', 'love'], ["n't", 'pretty'], ['truly', 'angel', 'best'], ['boy', 'happy'], ['heaven', 'blessed'], ["n't", 'believe', 'god', 'done'], ['given', 'life', 'one'], ["n't", 'lovely', 'made', 'love'], ["n't", 'lovely'], ['life', 'love'], ['life', 'aisha'], ['meaning', 'name'], ['londie', 'could', 'done'], ['without', 'conceived', 'one'], ['lovely', 'made', 'love']]
    

## 단어 집합 만들기( Python )


```python
from collections import Counter # 배열에 있는 원소의 개수를 세서 딕셔너리화 해준다.

# 2차원 배열 형식의 단어 집합을 1차원으로 풀어준다.
words = sum(sentences, [])
print(words)
```

    ["n't", 'lovely', "n't", 'wonderful', "n't", 'precious', 'less', 'one', 'minute', 'old', 'never', 'thought', 'love', 'making', 'one', 'lovely', "n't", 'lovely', 'made', 'love', "n't", 'pretty', 'truly', 'angel', 'best', 'boy', 'happy', 'heaven', 'blessed', "n't", 'believe', 'god', 'done', 'given', 'life', 'one', "n't", 'lovely', 'made', 'love', "n't", 'lovely', 'life', 'love', 'life', 'aisha', 'meaning', 'name', 'londie', 'could', 'done', 'without', 'conceived', 'one', 'lovely', 'made', 'love']
    


```python
vocab = Counter(words)
print(vocab)

```

    Counter({"n't": 8, 'lovely': 6, 'love': 5, 'one': 4, 'made': 3, 'life': 3, 'done': 2, 'wonderful': 1, 'precious': 1, 'less': 1, 'minute': 1, 'old': 1, 'never': 1, 'thought': 1, 'making': 1, 'pretty': 1, 'truly': 1, 'angel': 1, 'best': 1, 'boy': 1, 'happy': 1, 'heaven': 1, 'blessed': 1, 'believe': 1, 'god': 1, 'given': 1, 'aisha': 1, 'meaning': 1, 'name': 1, 'londie': 1, 'could': 1, 'without': 1, 'conceived': 1})
    


```python
print(vocab['lovely'])
```

    6
    


```python
# 빈도수가 높은 순서대로 정렬하기
vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)
print(vocab_sorted)
```

    [("n't", 8), ('lovely', 6), ('love', 5), ('one', 4), ('made', 3), ('life', 3), ('done', 2), ('wonderful', 1), ('precious', 1), ('less', 1), ('minute', 1), ('old', 1), ('never', 1), ('thought', 1), ('making', 1), ('pretty', 1), ('truly', 1), ('angel', 1), ('best', 1), ('boy', 1), ('happy', 1), ('heaven', 1), ('blessed', 1), ('believe', 1), ('god', 1), ('given', 1), ('aisha', 1), ('meaning', 1), ('name', 1), ('londie', 1), ('could', 1), ('without', 1), ('conceived', 1)]
    


```python
# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여
word2idx = {}

i = 0
for (word, frequency) in vocab_sorted :

  # 빈도수
  if frequency > 1:
    i = i + 1
    word2idx[word] = i

print(word2idx)

```

    {"n't": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, 'life': 6, 'done': 7}
    

단어를 모두 사용하는 것이 아닌, 빈도수 상위 top5만 사용하고 싶으면?


```python
vocab_size = 5
words_frequency = [w for w, c in word2idx.items() if c >= vocab_size + 1] # 인덱스 vocab_size를 초과하는 모든 단어의 목록 획득
for w in words_frequency:
  del word2idx[w]
print(word2idx)

```

    {"n't": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5}
    

실제 텍스트를 정수로 표현하기


```python
# OOV를 처리하기 위해 UNK 토큰 추가
word2idx['UNK'] = 6
print(word2idx)
```

    {"n't": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, 'UNK': 6}
    


```python
encoded = []
for s in sentences:
  temp = []
  for w in s:
    if w in word2idx:
      temp.append(word2idx[w])
    else:
      temp.append(word2idx['UNK'])
  encoded.append(temp)
print("변환 전 : {}".format(sentences[:5]))
print("변환 후 : {}".format(encoded[:5]))

```

    변환 전 : [["n't", 'lovely'], ["n't", 'wonderful'], ["n't", 'precious'], ['less', 'one', 'minute', 'old'], ['never', 'thought', 'love']]
    변환 후 : [[1, 2], [1, 6], [1, 6], [6, 4, 6, 6], [6, 6, 3]]
    

# Vocab & Integer Encoding을 Tensorflow로


```python
print(sentences)
```

    [["n't", 'lovely'], ["n't", 'wonderful'], ["n't", 'precious'], ['less', 'one', 'minute', 'old'], ['never', 'thought', 'love'], ['making', 'one', 'lovely'], ["n't", 'lovely', 'made', 'love'], ["n't", 'pretty'], ['truly', 'angel', 'best'], ['boy', 'happy'], ['heaven', 'blessed'], ["n't", 'believe', 'god', 'done'], ['given', 'life', 'one'], ["n't", 'lovely', 'made', 'love'], ["n't", 'lovely'], ['life', 'love'], ['life', 'aisha'], ['meaning', 'name'], ['londie', 'could', 'done'], ['without', 'conceived', 'one'], ['lovely', 'made', 'love']]
    

keras.preprocessing.text.Tokenizer 제공

* fit_on_texts를 사용하면 입력된 텍스트로부터 ** 단어 빈도수** 가 높은 순으로 낮은 정수 인덱스를 부여


```python
from tensorflow.keras.preprocessing.text import Tokenizer
```


```python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)

print(tokenizer.word_index)
```

    {"n't": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, 'life': 6, 'done': 7, 'wonderful': 8, 'precious': 9, 'less': 10, 'minute': 11, 'old': 12, 'never': 13, 'thought': 14, 'making': 15, 'pretty': 16, 'truly': 17, 'angel': 18, 'best': 19, 'boy': 20, 'happy': 21, 'heaven': 22, 'blessed': 23, 'believe': 24, 'god': 25, 'given': 26, 'aisha': 27, 'meaning': 28, 'name': 29, 'londie': 30, 'could': 31, 'without': 32, 'conceived': 33}
    


```python
# 각 단어의 빈도수 확인
print(tokenizer.word_counts)
```

    OrderedDict([("n't", 8), ('lovely', 6), ('wonderful', 1), ('precious', 1), ('less', 1), ('one', 4), ('minute', 1), ('old', 1), ('never', 1), ('thought', 1), ('love', 5), ('making', 1), ('made', 3), ('pretty', 1), ('truly', 1), ('angel', 1), ('best', 1), ('boy', 1), ('happy', 1), ('heaven', 1), ('blessed', 1), ('believe', 1), ('god', 1), ('done', 2), ('given', 1), ('life', 3), ('aisha', 1), ('meaning', 1), ('name', 1), ('londie', 1), ('could', 1), ('without', 1), ('conceived', 1)])
    


```python
# 정수 인코딩 수행
print(tokenizer.texts_to_sequences(sentences))
```

    [[1, 2], [1, 8], [1, 9], [10, 4, 11, 12], [13, 14, 3], [15, 4, 2], [1, 2, 5, 3], [1, 16], [17, 18, 19], [20, 21], [22, 23], [1, 24, 25, 7], [26, 6, 4], [1, 2, 5, 3], [1, 2], [6, 3], [6, 27], [28, 29], [30, 31, 7], [32, 33, 4], [2, 5, 3]]
    

상위 n개의 단어만 사용하기



```python
vocab_size = 5
tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개의 단어만 사용
tokenizer.fit_on_texts(sentences)
```


```python
print(tokenizer.word_index)
```

    {"n't": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, 'life': 6, 'done': 7, 'wonderful': 8, 'precious': 9, 'less': 10, 'minute': 11, 'old': 12, 'never': 13, 'thought': 14, 'making': 15, 'pretty': 16, 'truly': 17, 'angel': 18, 'best': 19, 'boy': 20, 'happy': 21, 'heaven': 22, 'blessed': 23, 'believe': 24, 'god': 25, 'given': 26, 'aisha': 27, 'meaning': 28, 'name': 29, 'londie': 30, 'could': 31, 'without': 32, 'conceived': 33}
    


```python
print(tokenizer.texts_to_sequences(sentences))
```

    [[1, 2], [1], [1], [4], [3], [4, 2], [1, 2, 5, 3], [1], [], [], [], [1], [4], [1, 2, 5, 3], [1, 2], [3], [], [], [], [4], [2, 5, 3]]
    


```python
# OOV, PADDING 같이 고려
tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')
tokenizer.fit_on_texts(sentences)
print(tokenizer.texts_to_sequences(sentences))
```

    [[2, 3], [2, 1], [2, 1], [1, 5, 1, 1], [1, 1, 4], [1, 5, 3], [2, 3, 6, 4], [2, 1], [1, 1, 1], [1, 1], [1, 1], [2, 1, 1, 1], [1, 1, 5], [2, 3, 6, 4], [2, 3], [1, 4], [1, 1], [1, 1], [1, 1, 1], [1, 1, 5], [3, 6, 4]]
    


```python
print("OOV의 인덱스 : {}".format(tokenizer.word_index['OOV']))
```

    OOV의 인덱스 : 1
    

# 한국어 문장 토큰화 및 정수 인코딩 하기


```python
!pip install konlpy
```

    Collecting konlpy
    [?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)
    [K     |████████████████████████████████| 19.4MB 1.2MB/s 
    [?25hCollecting JPype1>=0.7.0
    [?25l  Downloading https://files.pythonhosted.org/packages/fd/96/1030895dea70855a2e1078e3fe0d6a63dcb7c212309e07dc9ee39d33af54/JPype1-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (450kB)
    [K     |████████████████████████████████| 460kB 46.4MB/s 
    [?25hCollecting tweepy>=3.7.0
      Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl
    Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)
    Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)
    Collecting beautifulsoup4==4.6.0
    [?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)
    [K     |████████████████████████████████| 92kB 10.1MB/s 
    [?25hCollecting colorama
      Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl
    Requirement already satisfied: typing-extensions; python_version < "3.8" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)
    Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)
    Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)
    Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)
    Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == "socks" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)
    Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)
    Installing collected packages: JPype1, tweepy, beautifulsoup4, colorama, konlpy
      Found existing installation: tweepy 3.6.0
        Uninstalling tweepy-3.6.0:
          Successfully uninstalled tweepy-3.6.0
      Found existing installation: beautifulsoup4 4.6.3
        Uninstalling beautifulsoup4-4.6.3:
          Successfully uninstalled beautifulsoup4-4.6.3
    Successfully installed JPype1-1.1.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0
    


```python
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
%cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh
```

    Cloning into 'Mecab-ko-for-Google-Colab'...
    remote: Enumerating objects: 72, done.[K
    remote: Counting objects: 100% (72/72), done.[K
    remote: Compressing objects: 100% (67/67), done.[K
    remote: Total 72 (delta 31), reused 20 (delta 5), pack-reused 0[K
    Unpacking objects: 100% (72/72), done.
    /content/Mecab-ko-for-Google-Colab
    Installing konlpy.....
    Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)
    Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)
    Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)
    Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)
    Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)
    Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.1.2)
    Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)
    Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)
    Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)
    Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)
    Requirement already satisfied: typing-extensions; python_version < "3.8" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)
    Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)
    Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == "socks" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)
    Done
    Installing mecab-0.996-ko-0.9.2.tar.gz.....
    Downloading mecab-0.996-ko-0.9.2.tar.gz.......
    from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz
    --2020-11-12 00:17:01--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz
    Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c5:2ef4, 2406:da00:ff00::22c3:9b0a, ...
    Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=3w8FXIAwcPaP%2F%2F%2F5jr2h%2FIM8LBg%3D&Expires=1605141645&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]
    --2020-11-12 00:17:02--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=3w8FXIAwcPaP%2F%2F%2F5jr2h%2FIM8LBg%3D&Expires=1605141645&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None
    Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.239.155
    Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.239.155|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 1414979 (1.3M) [application/x-tar]
    Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’
    
    mecab-0.996-ko-0.9. 100%[===================>]   1.35M  1.24MB/s    in 1.1s    
    
    2020-11-12 00:17:04 (1.24 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]
    
    Done
    Unpacking mecab-0.996-ko-0.9.2.tar.gz.......
    Done
    Change Directory to mecab-0.996-ko-0.9.2.......
    installing mecab-0.996-ko-0.9.2.tar.gz........
    configure
    make
    make check
    make install
    ldconfig
    Done
    Change Directory to /content
    Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......
    from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz
    --2020-11-12 00:18:31--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz
    Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22e9:9f55, 2406:da00:ff00::6b17:d1f5, ...
    Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=NuZ9RacuGLb8MxGlauLYwHCIc1Q%3D&Expires=1605141333&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]
    --2020-11-12 00:18:32--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=NuZ9RacuGLb8MxGlauLYwHCIc1Q%3D&Expires=1605141333&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None
    Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.100.59
    Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.100.59|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 49775061 (47M) [application/x-tar]
    Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’
    
    mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  13.0MB/s    in 4.0s    
    
    2020-11-12 00:18:36 (11.9 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]
    
    Done
    Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......
    Done
    Change Directory to mecab-ko-dic-2.1.1-20180720
    Done
    installing........
    configure
    make
    make install
    apt-get update
    apt-get upgrade
    apt install curl
    apt install git
    bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)
    Done
    Successfully Installed
    Now you can use Mecab
    from konlpy.tag import Mecab
    mecab = Mecab()
    사용자 사전 추가 방법 : https://bit.ly/3k0ZH53
    


```python
import pandas as pd
import numpy as np
import urllib.request
from tensorflow.keras.preprocessing.text import Tokenizer
```


```python
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="ratings_test.txt")
```




    ('ratings_test.txt', <http.client.HTTPMessage at 0x7f422bb99e10>)




```python
train_data = pd.read_table("ratings_test.txt")
train_data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 50000 entries, 0 to 49999
    Data columns (total 3 columns):
     #   Column    Non-Null Count  Dtype 
    ---  ------    --------------  ----- 
     0   id        50000 non-null  int64 
     1   document  49997 non-null  object
     2   label     50000 non-null  int64 
    dtypes: int64(2), object(1)
    memory usage: 1.1+ MB
    


```python
train_data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>document</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6270596</td>
      <td>굳 ㅋ</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9274899</td>
      <td>GDNTOPCLASSINTHECLUB</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8544678</td>
      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6825595</td>
      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6723715</td>
      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
from konlpy.tag import Mecab
from konlpy.tag import Okt # 트위터
okt = Okt()
mecab = Mecab()
train_data['document'] = train_data['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
# 한글과 공백을 제외하고 모두 제거
train_data

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>document</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6270596</td>
      <td>굳 ㅋ</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8544678</td>
      <td>뭐야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6825595</td>
      <td>지루하지는 않은데 완전 막장임 돈주고 보기에는</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6723715</td>
      <td>만 아니었어도 별 다섯 개 줬을텐데 왜 로 나와서 제 심기를 불편하게 하죠</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>7898805</td>
      <td>음악이 주가 된 최고의 음악영화</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>49995</th>
      <td>4608761</td>
      <td>오랜만에 평점 로긴했네ㅋㅋ 킹왕짱 쌈뽕한 영화를 만났습니다 강렬하게 육쾌함</td>
      <td>1</td>
    </tr>
    <tr>
      <th>49996</th>
      <td>5308387</td>
      <td>의지 박약들이나 하는거다 탈영은 일단 주인공 김대희 닮았고 이등병 찐따</td>
      <td>0</td>
    </tr>
    <tr>
      <th>49997</th>
      <td>9072549</td>
      <td>그림도 좋고 완성도도 높았지만 보는 내내 불안하게 만든다</td>
      <td>0</td>
    </tr>
    <tr>
      <th>49998</th>
      <td>5802125</td>
      <td>절대 봐서는 안 될 영화 재미도 없고 기분만 잡치고 한 세트장에서 다 해먹네</td>
      <td>0</td>
    </tr>
    <tr>
      <th>49999</th>
      <td>6070594</td>
      <td>마무리는 또 왜이래</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>48417 rows × 3 columns</p>
</div>




```python
train_data['document'].nunique()
```




    (48418, 2)




```python
train_data.loc[train_data.document.isnull()]
train_data = train_data.dropna() # Null 값이 존재하는 행 제거
train_data['document'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())
train_data.head(10)
```

    id          0
    document    0
    label       0
    dtype: int64
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>document</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6270596</td>
      <td>굳 ㅋ</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8544678</td>
      <td>뭐야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6825595</td>
      <td>지루하지는 않은데 완전 막장임 돈주고 보기에는</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6723715</td>
      <td>만 아니었어도 별 다섯 개 줬을텐데 왜 로 나와서 제 심기를 불편하게 하죠</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>7898805</td>
      <td>음악이 주가 된 최고의 음악영화</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6315043</td>
      <td>진정한 쓰레기</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>6097171</td>
      <td>마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가고개를 젖게한다</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>8932678</td>
      <td>갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한남...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>6242223</td>
      <td>이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨  모든 사람이 그렇지는 않네</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>7462111</td>
      <td>괜찮네요오랜만포켓몬스터잼밌어요</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
from konlpy.tag import Okt # 트위터
okt = Okt()
```


```python
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']
train2 = []
for sentence in train_data['document']:
    temp_X = []
    temp_X = okt.morphs(sentence, stem=True) # 토큰화
    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거
    train2.append(temp_X)

```


```python
train2
```


```python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train2)

```


```python
print(tokenizer.word_index)
```


```python
# 각 단어의 빈도수 확인
print(tokenizer.word_counts)
```


```python
# 정수 인코딩 수행
print(tokenizer.texts_to_sequences(train2))
```


    Output hidden; open in https://colab.research.google.com to view.



```python
vocab_size = 5
tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개의 단어만 사용
tokenizer.fit_on_texts(train2)
```


```python
print(tokenizer.texts_to_sequences(train2))
```


```python
# OOV, PADDING 같이 고려
tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')
tokenizer.fit_on_texts(train2)
print(tokenizer.texts_to_sequences(train2))
```

# TF-IDF
* TF : Term Frequency
* IDF : Inverse Document Frequency


```python
# n번째 문서(document)에서 단어(term)가 등장한 횟수
def term_frequency(term, document):
  return document.count(term)

  # 단어(term)가 문서'들' (documents)에서 등장한 문서의 수
def document_frequency(term, documents):
  term_count = 0
  
  for document in documents:
    term_count += term in document
  return term_count
def inverse_document_frequency(term, documents):
  from math import log
  
  N = len(documents)
  df = document_frequency(term, documents)

  return log(N / (df + 1))

# idx번 문서에 term에 대한 tf-idf를 구해야 한다.
def tf_idf(term, documents, idx):
  document = documents[idx]
  return term_frequency(term, document) * inverse_document_frequency(term,documents)
```


```python
sample = "hello bye bye"
sample.count("bye")
term_frequency("bye", sample)
```




    2




```python
docs = [
  '동해 물과 백두산이 마르고 닳도록 하느님이 보우하사 우리나라 만세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 동해 가고 싶다',
  '남산 위에 저 소나무, 철갑을 두른 듯 바람 서리 불변함은 우리 기상일세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 소나무 이쁘다',
  '가을 하늘 공활한데 높고 구름 없이 밝은 달은 우리 가슴 일편단심일세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 가을 하늘 보고 싶다.',
  '이 기상과 이 마음으로 충성을 다하여 괴로우나 즐거우나 나라 사랑하세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 나라를 사랑하자'
]
```


```python
from konlpy.tag import Mecab
mecab = Mecab()

vocab = list(set(w for doc in docs for w in mecab.nouns(doc)))
vocab.sort()
```


```python
print(vocab)
```

    ['가슴', '가을', '강산', '구름', '기상', '길', '나라', '남산', '달', '대한', '데', '동해', '듯', '마음', '만세', '무궁화', '물', '바람', '백두산', '보우', '보전', '불변', '사람', '사랑', '삼천리', '서리', '소나무', '우리', '위', '일편단심', '철갑', '충성', '하느님', '하늘', '화려']
    


```python
# DTM(TF) 만들기
import pandas as pd

result = [] # 데이터 프레임 행 데이터를 담아낼 배열

for i in range(len(docs)):
  result.append([])
  d = docs[i] # 문서

  for j in range(len(vocab)):
    t = vocab[j]
    result[-1].append(term_frequency(t, d))

tf_ = pd.DataFrame(result, columns = vocab)
tf_


```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>가슴</th>
      <th>가을</th>
      <th>강산</th>
      <th>구름</th>
      <th>기상</th>
      <th>길</th>
      <th>나라</th>
      <th>남산</th>
      <th>달</th>
      <th>대한</th>
      <th>데</th>
      <th>동해</th>
      <th>듯</th>
      <th>마음</th>
      <th>만세</th>
      <th>무궁화</th>
      <th>물</th>
      <th>바람</th>
      <th>백두산</th>
      <th>보우</th>
      <th>보전</th>
      <th>불변</th>
      <th>사람</th>
      <th>사랑</th>
      <th>삼천리</th>
      <th>서리</th>
      <th>소나무</th>
      <th>우리</th>
      <th>위</th>
      <th>일편단심</th>
      <th>철갑</th>
      <th>충성</th>
      <th>하느님</th>
      <th>하늘</th>
      <th>화려</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
# TF-IDF 데이터 프레임 만들기
for j in range(len(vocab)):
    t = vocab[j]
    result[-1].append(term_frequency(t, docs))

idf_ = pd.DataFrame(result, columns = ["IDF"])
idf_

```


    ---------------------------------------------------------------------------

    AssertionError                            Traceback (most recent call last)

    /usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py in _list_to_arrays(data, columns, coerce_float, dtype)
        563     try:
    --> 564         columns = _validate_or_indexify_columns(content, columns)
        565         result = _convert_object_array(content, dtype=dtype, coerce_float=coerce_float)
    

    /usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py in _validate_or_indexify_columns(content, columns)
        688             raise AssertionError(
    --> 689                 f"{len(columns)} columns passed, passed data had "
        690                 f"{len(content)} columns"
    

    AssertionError: 1 columns passed, passed data had 70 columns

    
    The above exception was the direct cause of the following exception:
    

    ValueError                                Traceback (most recent call last)

    <ipython-input-22-3852f6f4bd4d> in <module>()
          4     result[-1].append(term_frequency(t, docs))
          5 
    ----> 6 idf_ = pd.DataFrame(result, columns = ["IDF"])
          7 idf_
    

    /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)
        507                     if is_named_tuple(data[0]) and columns is None:
        508                         columns = data[0]._fields
    --> 509                     arrays, columns = to_arrays(data, columns, dtype=dtype)
        510                     columns = ensure_index(columns)
        511 
    

    /usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py in to_arrays(data, columns, coerce_float, dtype)
        522         return [], []  # columns if columns is not None else []
        523     if isinstance(data[0], (list, tuple)):
    --> 524         return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)
        525     elif isinstance(data[0], abc.Mapping):
        526         return _list_of_dict_to_arrays(
    

    /usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py in _list_to_arrays(data, columns, coerce_float, dtype)
        565         result = _convert_object_array(content, dtype=dtype, coerce_float=coerce_float)
        566     except AssertionError as e:
    --> 567         raise ValueError(e) from e
        568     return result, columns
        569 
    

    ValueError: 1 columns passed, passed data had 70 columns


# Tensorflow로 BOW 구현하기


```python
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
```


```python
t = Tokenizer()
t.fit_on_texts(docs)
print(t.word_index)
```

    {'무궁화': 1, '삼천리': 2, '화려': 3, '강산': 4, '대한': 5, '사람': 6, '대한으로': 7, '길이': 8, '보전하세': 9, '동해': 10, '싶다': 11, '소나무': 12, '우리': 13, '가을': 14, '하늘': 15, '이': 16, '물과': 17, '백두산이': 18, '마르고': 19, '닳도록': 20, '하느님이': 21, '보우하사': 22, '우리나라': 23, '만세': 24, '가고': 25, '남산': 26, '위에': 27, '저': 28, '철갑을': 29, '두른': 30, '듯': 31, '바람': 32, '서리': 33, '불변함은': 34, '기상일세': 35, '이쁘다': 36, '공활한데': 37, '높고': 38, '구름': 39, '없이': 40, '밝은': 41, '달은': 42, '가슴': 43, '일편단심일세': 44, '보고': 45, '기상과': 46, '마음으로': 47, '충성을': 48, '다하여': 49, '괴로우나': 50, '즐거우나': 51, '나라': 52, '사랑하세': 53, '나라를': 54, '사랑하자': 55}
    


```python
print(t.texts_to_matrix(docs, mode = 'count')) # DTM
```

    [[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.
      1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
      1. 1. 1. 1. 1. 1. 1. 1.]]
    


```python
print(t.texts_to_matrix(docs, mode = 'tfidf').round(2))
```

    [[0.   0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 1.86 0.85 0.   0.
      0.   0.   0.   1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1  0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
     [0.   0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.   0.   1.86 0.85
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.1  1.1
      1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1  0.   0.   0.   0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
     [0.   0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.   0.85 0.   0.85
      1.86 1.86 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   1.1  1.1  1.1  1.1  1.1
      1.1  1.1  1.1  1.1  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
     [0.   0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.   0.   0.   0.
      0.   0.   1.86 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
      0.   0.   0.   0.   1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1  1.1 ]]
    


```python
print(t.texts_to_matrix(docs, mode = 'freq').round(2))
```

    [[0.   0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.1  0.05 0.   0.
      0.   0.   0.   0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
     [0.   0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.   0.   0.09 0.04
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.04 0.04
      0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.   0.   0.   0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
     [0.   0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.   0.04 0.   0.04
      0.08 0.08 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.04 0.04 0.04 0.04 0.04
      0.04 0.04 0.04 0.04 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
     [0.   0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.   0.   0.   0.
      0.   0.   0.1  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
      0.   0.   0.   0.   0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05]]
    


```python
print(t.texts_to_matrix(docs, mode = 'binary').round(2))
```

    [[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.
      1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0.]
     [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.
      0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
      1. 1. 1. 1. 1. 1. 1. 1.]]
    
