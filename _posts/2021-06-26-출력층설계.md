## 2021-06-26
# 신경망의 사용처
* 신경망은 분류와 회귀 모두 사용할 수 있습니다. 다만 둘 중 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라집니다! 일반적으로 회귀에서는 항등함수, 분류에서는 소프트맥스 함수를 사용합니다.

* 항등 함수와 소프트맥스 함수(softmax function) 구현하기
출력층에서 사용하는 활성화 함수인 항등함수(identity function)는 정말 쉽습니다. 일전에도 살펴 봤지만, 입력한 값을 그대로 출력값으로 내보내면 됩니다.

한편, 분류에서 사용 되어지는 소프트맥스의 식은 다음과 같습니다.


```python
import numpy as np

a = np.array([0.3, 2.9, 4.0]) # 출력층에서 계산된 3개의 값 - 출력층 뉴런이 3개

# 분자 구하기
exp_a = np.exp(a) # 모든 입력 신호에 대한 지수 함수 적용 ( 뉴런 각각의 분자 값 )

sum_exp_a = np.sum(exp_a) # 분모 구하기

# 신경망의 최종 예측은?
y = exp_a / sum_exp_a

print(y)
print(np.sum(y))
```

    [0.01821127 0.24519181 0.73659691]
    1.0
    


```python
# 나쁜 소프트 맥스
def softmax(a) :
  exp_a = np.exp(a) # 분자 구하기
  sum_exp_a = np.sum(exp_a) # 분모 구하기
  y = exp_a / sum_exp_a

  return y
```


```python
a = np.array([1010, 1000, 990])
print(np.exp(a) / np.sum(np.exp(a))) # 소프트맥스 계산
```

개선된 softmax 구현하기


```python
c = np.max(a) # 튜닝을 하기위한 상수 c는 입력값에서 제일 큰 값으로 선정하는 것이 일반적
print(a-c)
```

    [  0 -10 -20]
    


```python
# a - c를 softmax 함수에 들어갈 수 있게 한다.
print(np.exp(a-c) / np.sum(np.exp(a-c)))
```

    [9.99954600e-01 4.53978686e-05 2.06106005e-09]
    


```python
# 튜닝된 softmax
def softmax(a):
  c = np.max(a)
  exp_a = np.exp(a - c)

  sum_exp_a = np.sum(exp_a)

  y = exp_a / sum_exp_a
  return y
```

출력층 설계를 위해 필요한 것
* 출력 함수
* 분류 문제라면 출력층의 뉴런 개수 정하기


```python
# tensorflow의 keras를 이용해서 MNIST 불러오기
import tensorflow as tf
import matplotlib.pyplot as plt

%matplotlib inline
```


```python
# mnist 데이터셋 로딩
from tensorflow.keras import datasets
mnist = datasets.mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
    11493376/11490434 [==============================] - 0s 0us/step
    

어떤 종류의 문제를 풀던 항상 데이터의 shape부터 확인할 것


```python
X_train.shape, y_train.shape
```




    ((60000, 28, 28), (60000,))




```python
# 이미지를 시각화 해보기
image = X_train[0]
image.shape
```




    (28, 28)




```python
plt.imshow(image, 'gray')
plt.show()
```


    
![output_14_0](https://user-images.githubusercontent.com/69663368/123517561-a091f000-d6dc-11eb-8c8a-183d4528672b.png)

    



```python
plt.imshow(image, 'gray')
plt.title(y_train[0])
plt.show()
```


    
![output_15_0](https://user-images.githubusercontent.com/69663368/123517568-ae477580-d6dc-11eb-8a95-f3a52e70eb3c.png)
    



```python
plt.imshow(X_train[1], 'gray')
plt.title(y_train[1])
plt.show()
```


![output_16_0](https://user-images.githubusercontent.com/69663368/123517581-bb646480-d6dc-11eb-94dc-3f5bbdf08868.png)
    


신경망에 넣을 준비 - 우리가 준비할 신경망은 Fully Connected Layer

데이터가 1차원
 데이터 형식으로 쭉 퍼져 있어야 한다. (flatten)





```python
image = image.reshape(-1)
image.shape
```




    (784,)




```python
image_bokgu = image.reshape(28, 28)
plt.imshow(image_bokgu, 'gray')
plt.show()
```


    
![output_19_0](https://user-images.githubusercontent.com/69663368/123517589-c7502680-d6dc-11eb-9297-462854c11e04.png)
    



```python
image_size = X_test.shape[0]
image_size
```




    10000




```python
X_test_reshape = X_test.reshape(image_size, -1)
X_test_reshape.shape
```




    (10000, 784)



MNIST 신경망 만들기 - Fully Connected 방식


```python
# 활성화 함수 구현 ( 시그모이드 )
def sigmoid(x):
  return 1 / (1 + np.exp(-x))

# 테스트 데이터 가져오기
def get_test_data():
  (X_train, y_train), (X_test, y_test) = mnist.load_data()

  X_test_reshape = X_test.reshape(image_size, -1)
  return X_test_reshape, y_test
# 훈련된 신경망 가져오기
def init_network():
  import pickle
  with open("./sample_weight.pkl", 'rb') as f:
    network = pickle.load(f)

  return network

# 예측 함수 만들기
def predict(network, x):
  W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']


  # 구현시에 나는 대부분의 오류는 데이터의 형상(shape) 때문이다.

  # 1. 각 층은 입력되는 값과 해당층의 가중치를 곱하고 편향을 더한다.

  # 2. (1) 에 의해서 계산된 값에 각 층의 활성화 함수를 씌워주고 다음 층으로 넘긴다

  # Layer1 계산 (입력 : x)
  z1 = np.dot(x, W1) + b1
  a1 = sigmoid(z1)

  # Layer2 계산 (입력 : a1)
  z2 = np.dot(a1, W2) + b2
  a2 = sigmoid(z2)

  # Layer3 계산 (입력 : a2) - 출력층이기 때문에 활성화 함수로 softmax를 사용한다.
  z3 = np.dot(a2, W3) + b3
  y = softmax(z3)

  return y




```


```python
network = init_network() 
W1, W2, W3 = network['W1'], network['W2'], network['W3']
b1, b2, b3 = network['b1'], network['b2'], network['b3']

W1.shape, W2.shape, W3.shape
```




    ((784, 50), (50, 100), (100, 10))




```python
X, y = get_test_data()
X.shape, y.shape
```




    ((10000, 784), (10000,))




```python
X, y = get_test_data()
network = init_network()

accuracy_count = 0 # 맞춘 개수를 저장 ( 정답을 맞추면 1 증가 )

# 사진을 한장씩 꺼내기
for i in range(len(X)):
  pred = predict(network, X[i])
  pred = np.argmax(pred) # 확률이 가장 높은 원소의 인덱스

  if pred == y[i]:
    accuracy_count += 1

print(float(accuracy_count) / len(X))
```

    /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp
      This is separate from the ipykernel package so we can avoid doing imports until
    

    0.9207
    

위 처럼 했을 때의 단점 - 10000장을 한장씩 검사하고 있다.


```python
X, y = get_test_data()

network = init_network()

# 배치란? 데이터의 묶음이다.
# batch_size : 1 배치당 들어있어야 하는 데이터의 개수
# ex) 60000개의 데이터를 batch_size 100으로 묶으면 600개의 배치데이터가 생긴다.
# 참고로 배치를 활용해 모든 데이터를 모두 학습시켰으면, 1 에폭(epoch)이라고 한다.
batch_size = 100
accuracy_cnt = 0

for i in range(0, len(X), batch_size):
  X_batch = X[i : i + batch_size] # 순서대로 100개씩 데이터를 쪼갬
  y_batch = predict(network, X_batch) # 데이터를 100개씩 예측

  p = np.argmax(y_batch, axis=1) # 100개의 예측 데이터에서 가장 높은 값의 인덱스를 추출
  accuracy_cnt += np.sum( p == y[i : i + batch_size]) # 100개씩 정답의 합을 구함

print(float(accuracy_cnt) / len(X))
```
