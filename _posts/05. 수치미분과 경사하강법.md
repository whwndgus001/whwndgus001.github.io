```python
import numpy as np

# 나쁜 미분의 구현
# 함수 f(x)에서, x가 아주 작은 수인 h만큼 변경 되었을 때의 변화량을 구하는 것
def numerical_diff(f, x):
  h = 10e-50
  return (f(x+h) - f(x))  / h
```


```python
print(np.float32(1e-50)) # 너무 작아서 컴퓨터 시스템에서는 그냥 0으로 표현
```

    0.0
    


```python
def numerical_diff(f, x):
  h = 1e-4 # 0.0001
  return (f(x + h) - f(x - h)) / (2 * h)
```

# 편미분



```python
# x0 = 3, x1 = 4일때 함수 f에 대한 x0의 편미분을 구하기
# x1을 4로 대입했을 때의 x0의 미분값을 구하는 것과 동일하다.

def func_1(x0):
  return x0 ** 2 + 4.0 **  2

print(numerical_diff(func_1, 3.0))
```

    6.00000000000378
    


```python
def func_2(x1):
  return 7.0 ** 2 + x1 ** 2 

print(numerical_diff(func_2, 3.0))
```

    5.9999999999860165
    


```python
def numerical_gradient(f, x):
  h = 1e-4 # 0.0001
  grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성
  # 각 변수에 대한 미분값 구하기
  for idx in range(x.size):
      tmp_val = x[idx]
      
      # f(x+h) 계산
      x[idx] = float(tmp_val) + h
      fxh1 = f(x)
      
      # f(x-h) 계산
      x[idx] = tmp_val - h 
      fxh2 = f(x) 
      # 각 편미분의 기울기를 저장
      #ex) x0에 대한 편미분을 구하고 해당 기울기를 해당 인덱스에 저장
      #x1에 대한 편미분을 구하고 해당 기울기를 해당 인덱스에 저장
      grad[idx] = (fxh1 - fxh2) / (2*h)
      x[idx] = tmp_val # 값 복원
      
  return grad
```


```python
def function_2(x):
  return x[0] ** 2 + x[1] ** 2
```


```python
result = numerical_gradient(function_2, np.array([3.0, 4.0]))
print("x = [3, 4] 일 때의 기울기 배열 : {}".format(result))

result = numerical_gradient(function_2, np.array([0.0, 2.0]))
print("x = [0, 2] 일 때의 기울기 배열 : {}".format(result))

result = numerical_gradient(function_2, np.array([3.0, 0.0]))
print("x = [3, 0] 일 때의 기울기 배열 : {}".format(result))
```

    x = [3, 4] 일 때의 기울기 배열 : [6. 8.]
    x = [0, 2] 일 때의 기울기 배열 : [0. 4.]
    x = [3, 0] 일 때의 기울기 배열 : [6. 0.]
    

경사하강법 구현하기


```python
'''
  f : 함수()
  init_x : 시작 지점
  lr : 학습률(learning rate)
  step_num : 학습 횟수 ( 경사하강법을 진행할 횟수 )
'''
# coding: utf-8
import numpy as np
import matplotlib.pylab as plt


def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    x_history = []

    for i in range(step_num):
        x_history.append( x.copy() )

        grad = numerical_gradient(f, x)
        x -= lr * grad

    return x, np.array(x_history)

init_x = np.array([-3.0, 4.0])    

lr = 0.1
step_num = 20
x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)

plt.plot( [-5, 5], [0,0], '--b')
plt.plot( [0,0], [-5, 5], '--b')
plt.plot(x_history[:,0], x_history[:,1], 'o')

plt.xlim(-3.5, 3.5)
plt.ylim(-4.5, 4.5)
plt.xlabel("X0")
plt.ylabel("X1")
plt.show()


```


    
![png](output_10_0.png)
    



```python
init_x = np.array([-3.0, 4.0])
print("최솟값 탐색 결과 : {}".format(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)))
```

    최솟값 탐색 결과 : (array([-6.11110793e-10,  8.14814391e-10]), array([[-3.00000000e+00,  4.00000000e+00],
           [-2.40000000e+00,  3.20000000e+00],
           [-1.92000000e+00,  2.56000000e+00],
           [-1.53600000e+00,  2.04800000e+00],
           [-1.22880000e+00,  1.63840000e+00],
           [-9.83040000e-01,  1.31072000e+00],
           [-7.86432000e-01,  1.04857600e+00],
           [-6.29145600e-01,  8.38860800e-01],
           [-5.03316480e-01,  6.71088640e-01],
           [-4.02653184e-01,  5.36870912e-01],
           [-3.22122547e-01,  4.29496730e-01],
           [-2.57698038e-01,  3.43597384e-01],
           [-2.06158430e-01,  2.74877907e-01],
           [-1.64926744e-01,  2.19902326e-01],
           [-1.31941395e-01,  1.75921860e-01],
           [-1.05553116e-01,  1.40737488e-01],
           [-8.44424930e-02,  1.12589991e-01],
           [-6.75539944e-02,  9.00719925e-02],
           [-5.40431955e-02,  7.20575940e-02],
           [-4.32345564e-02,  5.76460752e-02],
           [-3.45876451e-02,  4.61168602e-02],
           [-2.76701161e-02,  3.68934881e-02],
           [-2.21360929e-02,  2.95147905e-02],
           [-1.77088743e-02,  2.36118324e-02],
           [-1.41670994e-02,  1.88894659e-02],
           [-1.13336796e-02,  1.51115727e-02],
           [-9.06694365e-03,  1.20892582e-02],
           [-7.25355492e-03,  9.67140656e-03],
           [-5.80284393e-03,  7.73712525e-03],
           [-4.64227515e-03,  6.18970020e-03],
           [-3.71382012e-03,  4.95176016e-03],
           [-2.97105609e-03,  3.96140813e-03],
           [-2.37684488e-03,  3.16912650e-03],
           [-1.90147590e-03,  2.53530120e-03],
           [-1.52118072e-03,  2.02824096e-03],
           [-1.21694458e-03,  1.62259277e-03],
           [-9.73555661e-04,  1.29807421e-03],
           [-7.78844529e-04,  1.03845937e-03],
           [-6.23075623e-04,  8.30767497e-04],
           [-4.98460498e-04,  6.64613998e-04],
           [-3.98768399e-04,  5.31691198e-04],
           [-3.19014719e-04,  4.25352959e-04],
           [-2.55211775e-04,  3.40282367e-04],
           [-2.04169420e-04,  2.72225894e-04],
           [-1.63335536e-04,  2.17780715e-04],
           [-1.30668429e-04,  1.74224572e-04],
           [-1.04534743e-04,  1.39379657e-04],
           [-8.36277945e-05,  1.11503726e-04],
           [-6.69022356e-05,  8.92029808e-05],
           [-5.35217885e-05,  7.13623846e-05],
           [-4.28174308e-05,  5.70899077e-05],
           [-3.42539446e-05,  4.56719262e-05],
           [-2.74031557e-05,  3.65375409e-05],
           [-2.19225246e-05,  2.92300327e-05],
           [-1.75380196e-05,  2.33840262e-05],
           [-1.40304157e-05,  1.87072210e-05],
           [-1.12243326e-05,  1.49657768e-05],
           [-8.97946606e-06,  1.19726214e-05],
           [-7.18357285e-06,  9.57809713e-06],
           [-5.74685828e-06,  7.66247770e-06],
           [-4.59748662e-06,  6.12998216e-06],
           [-3.67798930e-06,  4.90398573e-06],
           [-2.94239144e-06,  3.92318858e-06],
           [-2.35391315e-06,  3.13855087e-06],
           [-1.88313052e-06,  2.51084069e-06],
           [-1.50650442e-06,  2.00867256e-06],
           [-1.20520353e-06,  1.60693804e-06],
           [-9.64162827e-07,  1.28555044e-06],
           [-7.71330261e-07,  1.02844035e-06],
           [-6.17064209e-07,  8.22752279e-07],
           [-4.93651367e-07,  6.58201823e-07],
           [-3.94921094e-07,  5.26561458e-07],
           [-3.15936875e-07,  4.21249167e-07],
           [-2.52749500e-07,  3.36999333e-07],
           [-2.02199600e-07,  2.69599467e-07],
           [-1.61759680e-07,  2.15679573e-07],
           [-1.29407744e-07,  1.72543659e-07],
           [-1.03526195e-07,  1.38034927e-07],
           [-8.28209562e-08,  1.10427942e-07],
           [-6.62567649e-08,  8.83423532e-08],
           [-5.30054119e-08,  7.06738826e-08],
           [-4.24043296e-08,  5.65391061e-08],
           [-3.39234636e-08,  4.52312849e-08],
           [-2.71387709e-08,  3.61850279e-08],
           [-2.17110167e-08,  2.89480223e-08],
           [-1.73688134e-08,  2.31584178e-08],
           [-1.38950507e-08,  1.85267343e-08],
           [-1.11160406e-08,  1.48213874e-08],
           [-8.89283245e-09,  1.18571099e-08],
           [-7.11426596e-09,  9.48568795e-09],
           [-5.69141277e-09,  7.58855036e-09],
           [-4.55313022e-09,  6.07084029e-09],
           [-3.64250417e-09,  4.85667223e-09],
           [-2.91400334e-09,  3.88533778e-09],
           [-2.33120267e-09,  3.10827023e-09],
           [-1.86496214e-09,  2.48661618e-09],
           [-1.49196971e-09,  1.98929295e-09],
           [-1.19357577e-09,  1.59143436e-09],
           [-9.54860614e-10,  1.27314749e-09],
           [-7.63888491e-10,  1.01851799e-09]]))
    

학습률에 대한 결과 확인하기


```python
init_x = np.array([-3.0, 4.0])

result, _ = gradient_descent(function_2, init_x = init_x, lr = 10.0, step_num = 100)
print("학습률이 10.0일 때 : {}".format(result)) # 발산
```

    학습률이 10.0일 때 : [-2.58983747e+13 -1.29524862e+12]
    

학습률이 너무 크면 최소지점을 찾지 못하고 발산하는 것을 알 수 있다.


```python
init_x = np.array([-3.0, 4.0])
result, _ = gradient_descent(function_2, init_x = init_x, lr=1e-10, step_num = 100)
print("학습률이 1e-10일 때: {}".format(result))
```

    학습률이 1e-10일 때: [-2.99999994  3.99999992]
    

학습률이 너무 작으면 값의 갱신이 거의 이뤄지지 않는다.

학습률의 기본값은 거의 0.01을 놓고 사용한다.


```python
%cd common
!unzip common.zip
```

    /content/common
    Archive:  common.zip
      inflating: functions.py            
      inflating: gradient.py             
      inflating: layers.py               
      inflating: multi_layer_net.py      
      inflating: multi_layer_net_extend.py  
      inflating: optimizer.py            
      inflating: trainer.py              
      inflating: util.py                 
     extracting: __init__.py             
    


```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient
```


```python
class SimpleNet:
  def __init__(self):
    # 신경망의 초기화 과정
    # 1) 정규분포 랜덤 * 0.01 ( 일반적인 케이스 )
    # 2) 카이밍 히 초기값 ( He 초깃값 )
    # 3) 사비에르 초깃값 ( Xavier 초깃값 )

    self.W = np.random.randn(2, 3) # 가중치를 랜덤하게 정규분포로 초기화 한다.
    
  def predict(self, x):
    return np.dot(x, self.W)
  def loss(self, x, t):
    z = self.predict(x)
    y = softmax(z) # 출력값 구하기

    loss = cross_entropy_error(y, t) # Loss값 구하기, 실제값과 예측값이 동시에 들어간다.

    return loss

```


```python
net = SimpleNet()
print("가중치 : {}".format(net.W))
```

    가중치 : [[ 0.97022852  0.76192143  0.1526186 ]
     [-1.37022125 -0.84388523  0.11582853]]
    


```python
x = np.array([0.6, 0.9])
p = net.predict(x)

print("예측된 값 : {}".format(p))
```

    예측된 값 : [-0.65106201 -0.30234384  0.19581683]
    


```python
print("최댓값의 인덱스 : {}".format(np.argmax(p))) # 최댓값의 인덱스 확인
```

    최댓값의 인덱스 : 2
    


```python
t = np.array([1, 0, 0]) # 정답이 0일 때
t_error = np.array([0, 0, 1]) # 정답이 2일 때

print("정답이 0 일때의 LOSS : {}".format(net.loss(x, t)))
print("정답이 2 일때의 LOSS : {}".format(net.loss(x, t_error)))
```

    정답이 0 일때의 LOSS : 1.5580610992118333
    정답이 2 일때의 LOSS : 0.7111825210316501
    


```python
t = np.array([0, 0, 1]) # 정답이 2일 때 ( 예측이 틀렸다고 판단. )

def f(W):
  return net.loss(x, t)
```


```python
# LOSS를 구하는 함수 f에 대한 모든 W들의 기울기 구하기.
# W의 각 원소에 대해 편미분이 일어난다.
dW = numerical_gradient(f, net.W) 
print(dW)
```

    [[ 0.12632625  0.17903585 -0.3053621 ]
     [ 0.18948938  0.26855377 -0.45804315]]
    


```python
w11 h만큼 증가하면 LOSS가 0.12h만큼 증가한다.
w31 h만큼 증가하면 LOSS가 0.30h만큼 감소한다.
```


```python
import sys, os
sys.path.append(os.pardir)

from common.functions import *
from common.gradient import  numerical_gradient
```


```python
'''
TwoLayerNet : 2층 짜리 신경망 - 은닉층 1개, 출력층 1개
- 마음대로 Hidden Unit, Output Size를 조절할 수 있게 만들 것임.
'''

class TwoLayerNet:
  # 초기화시 필요한 것들
  # 1. 입력데이터(x)의 size
  #   -n개(x의 개수)의 입력을 받을 W의 개수를 파악해야 하기 때문에
  #   -MNIST의 경우에는 28*28개의 입력을 받는다.

  # 2. hidden unit의 크기
  # 3. output size
  # 4. 가중치 정규분포 초기화 값( 정규분포 랜덤값에 곱할 값 )
  def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
    # 가중치 초기화
    self.params = {}

    # 1층 매개변수 마련하기
    self.params["W1"] = weight_init_std * np.random.randn(input_size, hidden_size)
    self.params['b1'] = np.zeros(hidden_size)
    
    # 2층(출력층) 매개변수 마련하기
    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
    self.params['b2'] = np.zeros(output_size)

    pass
  def predict(self, x):
    W1, W2, b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']
    # 1층 계산
    z1 = np.dot(x, W1) + b1
    a1 = sigmoid(z1)
    # 출력층 계산
    z2 = np.dot(a1, W2) + b2
    y = softmax(z2)

    return y


  def loss(self, x, t):
    # LOSS를 구하기 위해서는 예측(predict)부터 한다.
    y = self.predict(x)

    return cross_entropy_error(y, t)

  def accuaracy(self, x, t):
    y = self.predict(x)
    y = np.argmax(y, axis = 1)
    t = np.aramax(t, axis = 1)

    accuracy = np.sum(y==t) / float(x.shape[0])
    return accuracy
  # 신경망의 모든 매개변수들에 대한 미분값(기울기) 구하기 - 수치미분을 활용합니다.
  def numerical_gradient(self, x, t):
    print("미분 ㄱㄱ")
    loss_W = lambda W : self.loss(x, t)

    # 각 층에서 구해지는 기울기를 저장할 딕셔너리
    grads = {}

    # 1층 매개변수들의 기울기 구하기 ( LOSS에 대한 W1, b1의 기울기 )
    grads["W1"] = numerical_gradient(loss_W, self.params['W1'])
    grads["b1"] = numerical_gradient(loss_W, self.params['b1'])

    # 2층 매개변수들의 기울기 구하기 ( LOSS에 대한 W2, b2의 기울기)
    grads["W2"] = numerical_gradient(loss_W, self.params['W2'])
    grads["b2"] = numerical_gradient(loss_W, self.params['b2'])

    return grads
    
```


```python
# 신경망 생성하기
input_size = 28 * 28
hidden_size = 100
output_size = 10

net = TwoLayerNet(input_size = input_size, hidden_size = hidden_size, output_size = output_size)
```


```python
net.params['W1'].shape, net.params['W2'].shape
```




    ((784, 100), (100, 10))



총 매개변수의 갯수는?
* 79510개
(78400 + 100) + (1000 + 10)


```python
# 신경망이 잘 돌아가는지 임의의 데이터를 집어 넣어서 테스트 해보기
x = np.random.rand(100,784) # 이미지 100장 미니배치를 임의로 만들었음.
y = net.predict(x)
```


```python
x.shape, y.shape
```




    ((100, 784), (100, 10))



# mnist 데이터셋 불러오기


```python
from tensorflow.keras import datasets
mnist = datasets.mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()
```


```python
X_train.shape, y_train.shape
```




    ((60000, 28, 28), (60000,))




```python
X_train = X_train.reshape(60000, -1)
X_train.shape, y_train.shape
```




    ((60000, 784), (60000,))




```python
y_train_step1 = y_train.reshape(-1, 1)
y_train_step1.shape
```




    (60000, 1)




```python
from sklearn.preprocessing import OneHotEncoder
y_train_dummy = OneHotEncoder().fit_transform( y_train.reshape(-1, 1))
y_train_dummy = y_train_dummy.toarray()

y_test_dummy = OneHotEncoder().fit_transform(y_test.reshape(-1, 1))
y_test_dummy = y_test_dummy.toarray()
```


```python
y_train_dummy.shape, y_test_dummy.shape
```




    ((60000, 10), (10000, 10))




```python
X_train = X_train.reshape(X_train.shape[0], -1)
X_train = X_train / 255.0 # 정규화
X_train.shape
```




    (60000, 784)




```python
X_test = X_test.reshape(X_test.shape[0], -1)
X_test = X_test / 255.0
X_test.shape
```




    (10000, 784)



# 훈련
* 미니 배치 선정
* 반복 횟수 설정
* 학습률 선정


```python
from tqdm import tqdm_notebook

# 반복 횟수 설정
iter_nums = 10000

# 미니배치 획득 과정
train_size = X_train.shape[0]
batch_size = 100

# 학습률 설정
learning_rate = 0.1

network = TwoLayerNet(input_size, hidden_size = 100, output_size = 10)

for i in tqdm_notebook(range(iter_nums)):
  # 미니 배치 인덱스 선정하기
  batch_mask = np.random.choice(train_size, batch_size)

  X_batch = X_train[batch_mask]
  t_batch = y_train_dummy[batch_mask]

  # 각 배치 마다의 기울기를 계산
  # network의 numerical_gradient에서 하는 일
  # 1. 예측
  # 2. cross_entropy_error를 이용한 LOSS 구하기
  # 3. 구해진 LOSS값을 이용해 미분을 수행해서 각 층의 매개변수 기울기를 저장
  
  grads = network.numerical_gradient(X_batch, t_batch)

  for key in grads.keys():
    # 각 매개변수마다 경사하강법 수행하기
    network.params[key] -= learning_rate * grads[key]

  # 갱신된 Loss 확인하기
  loss = network.loss(X_batch, t_batch)
  print("Step {} -> Loss : {}".format(i, loss))



```

    /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0
    Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`
      from ipykernel import kernelapp as app
    


    HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))


    미분 ㄱㄱ
    Step 0 -> Loss : 2.2792661713017006
    미분 ㄱㄱ
    


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-115-fbd2e1bb91d5> in <module>()
         26   # 3. 구해진 LOSS값을 이용해 미분을 수행해서 각 층의 매개변수 기울기를 저장
         27 
    ---> 28   grads = network.numerical_gradient(X_batch, t_batch)
         29 
         30   for key in grads.keys():
    

    <ipython-input-113-d7dc44224bab> in numerical_gradient(self, x, t)
         60 
         61     # 1층 매개변수들의 기울기 구하기 ( LOSS에 대한 W1, b1의 기울기 )
    ---> 62     grads["W1"] = numerical_gradient(loss_W, self.params['W1'])
         63     grads["b1"] = numerical_gradient(loss_W, self.params['b1'])
         64 
    

    /content/common/gradient.py in numerical_gradient(f, x)
         44 
         45         x[idx] = tmp_val - h
    ---> 46         fxh2 = f(x) # f(x-h)
         47         grad[idx] = (fxh1 - fxh2) / (2*h)
         48 
    

    <ipython-input-113-d7dc44224bab> in <lambda>(W)
         54   def numerical_gradient(self, x, t):
         55     print("미분 ㄱㄱ")
    ---> 56     loss_W = lambda W : self.loss(x, t)
         57 
         58     # 각 층에서 구해지는 기울기를 저장할 딕셔너리
    

    <ipython-input-113-d7dc44224bab> in loss(self, x, t)
         40   def loss(self, x, t):
         41     # LOSS를 구하기 위해서는 예측(predict)부터 한다.
    ---> 42     y = self.predict(x)
         43 
         44     return cross_entropy_error(y, t)
    

    <ipython-input-113-d7dc44224bab> in predict(self, x)
         29     W1, W2, b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']
         30     # 1층 계산
    ---> 31     z1 = np.dot(x, W1) + b1
         32     a1 = sigmoid(z1)
         33     # 출력층 계산
    

    <__array_function__ internals> in dot(*args, **kwargs)
    

    KeyboardInterrupt: 



```python

```
