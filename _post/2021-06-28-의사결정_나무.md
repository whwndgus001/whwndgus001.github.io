## 2020-06-28
```python
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier
```


```python
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data,
                                                    cancer.target,
                                                    stratify = cancer.target,
                                                    random_state = 0)


# tree 계열
tree = DecisionTreeClassifier(random_state = 0)
tree.fit(X_train, y_train)
```




    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                           max_depth=None, max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, presort='deprecated',
                           random_state=0, splitter='best')




```python
print("훈련 세트 정확도 : {:.3f}".format(tree.score(X_train, y_train)))
print("테스트 세트 정확도 : {:.3f}".format(tree.score(X_test, y_test)))
```

    훈련 세트 정확도 : 1.000
    테스트 세트 정확도 : 0.902
    

복잡도를 낮추기 위해 나무의 최대 깊이 제한하기
* 사이킷런은 사전 가지치기 기법을 사용


```python
# 범용적으로 max_depth를 사용하면 좋다.
tree = DecisionTreeClassifier(max_depth = 4, random_state = 0)
tree.fit(X_train, y_train)

print("훈련 세트 정확도 : {:.3f}".format(tree.score(X_train, y_train)))
print("테스트 세트 정확도 : {:.3f}".format(tree.score(X_test, y_test)))
```

    훈련 세트 정확도 : 0.986
    테스트 세트 정확도 : 0.909
    


```python
import matplotlib.pyplot as plt
import numpy as np
def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.barh(range(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), cancer.feature_names)
    plt.xlabel("Feature Importance")
    plt.ylabel("Feature")
    plt.ylim(-1, n_features)
    plt.show

plot_feature_importances_cancer(tree)
```


    
![output_5_0](https://user-images.githubusercontent.com/69663368/123610054-32177400-d83b-11eb-86b3-9dc618c131f5.png)
    


feature_importance_의 값이 낮다고 해서 중요하지 않은건 아니다!

단순히 모델이 선택을 안했을 수도 있기 때문에~

랜덤 포레스트


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_moons

X, y = make_moons(n_samples = 100, noise = 0.25, random_state = 3)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)

forest =  RandomForestClassifier(n_estimators = 5, random_state = 2)
forest.fit(X_train, y_train)
```




    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=5,
                           n_jobs=None, oob_score=False, random_state=2, verbose=0,
                           warm_start=False)




```python
!pip install mglearn
```

    Collecting mglearn
    [?25l  Downloading https://files.pythonhosted.org/packages/65/38/8aced26fce0b2ae82c3c87cd3b6105f38ca6d9d51704ecc44aa54473e6b9/mglearn-0.1.9.tar.gz (540kB)
    [K     |████████████████████████████████| 542kB 2.8MB/s 
    [?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mglearn) (1.18.5)
    Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from mglearn) (3.2.2)
    Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.22.2.post1)
    Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mglearn) (1.1.3)
    Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from mglearn) (7.0.0)
    Requirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.10.0)
    Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from mglearn) (2.4.1)
    Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.17.0)
    Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (1.2.0)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.4.7)
    Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.8.1)
    Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->mglearn) (1.4.1)
    Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mglearn) (2018.9)
    Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler->mglearn) (1.15.0)
    Building wheels for collected packages: mglearn
      Building wheel for mglearn (setup.py) ... [?25l[?25hdone
      Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582639 sha256=5356d1d7e9f7d6f704207e6f564908c0ca3fa3c5405d4ba13d939fa054c71f9b
      Stored in directory: /root/.cache/pip/wheels/eb/a6/ea/a6a3716233fa62fc561259b5cb1e28f79e9ff3592c0adac5f0
    Successfully built mglearn
    Installing collected packages: mglearn
    Successfully installed mglearn-0.1.9
    


```python
import mglearn
fig, axes = plt.subplots(2, 3, figsize=(20, 10))
for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):
    ax.set_title("Tree {}".format(i))![Uploading output_10_1.png…]()

    mglearn.plots.plot_tree_partition(X, y, tree, ax=ax)

mglearn.plots.plot_2d_separator(forest, X, fill=True, ax=axes[-1, -1], alpha=.4)
axes[-1, -1].set_title("Random Forest")
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
```




    [<matplotlib.lines.Line2D at 0x7f2c47157898>,
     <matplotlib.lines.Line2D at 0x7f2c471a3668>]




    
![output_10_1](https://user-images.githubusercontent.com/69663368/123610470-93d7de00-d83b-11eb-9ce9-7f2676bc04f2.png)
    



```python
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,random_state = 0)
forest =  RandomForestClassifier(n_estimators = 100 , random_state = 0)
forest.fit(X_train, y_train)

print("훈련 세트 정확도 : {:.3f}".format(forest.score(X_train, y_train)))
print("테스트 세트 정확도 : {:.3f}".format(forest.score(X_test, y_test)))

```

    훈련 세트 정확도 : 1.000
    테스트 세트 정확도 : 0.972
    


```python
plot_feature_importances_cancer(forest)
```


    
![output_12_0](https://user-images.githubusercontent.com/69663368/123610578-af42e900-d83b-11eb-8520-cdb0d654cb2a.png)![output_15_0 2](https://user-images.githubusercontent.com/69663368/123610648-bd910500-d83b-11eb-9a1d-387c23326131.png)
    


```python
from sklearn.ensemble import GradientBoostingClassifier

gbrt = GradientBoostingClassifier(random_state = 0)
gbrt.fit(X_train, y_train)

print("훈련 세트 정확도 : {:.3f}".format(gbrt.score(X_train, y_train)))
print("테스트 세트 정확도 : {:.3f}".format(gbrt.score(X_test, y_test)))
```

    훈련 세트 정확도 : 1.000
    테스트 세트 정확도 : 0.965
    


```python
gbrt = GradientBoostingClassifier(random_state = 0, max_depth = 1)
gbrt.fit(X_train, y_train)

print("훈련 세트 정확도 : {:.3f}".format(gbrt.score(X_train, y_train)))
print("테스트 세트 정확도 : {:.3f}".format(gbrt.score(X_test, y_test)))
```

    훈련 세트 정확도 : 0.991
    테스트 세트 정확도 : 0.972
    


```python
plot_feature_importances_cancer(gbrt)
```


    
![Uploading output_15_0.2.png…]()
    

